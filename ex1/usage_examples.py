#!/usr/bin/env python3
"""
×“×•×’×××•×ª ×©×™××•×© ××¢×©×™×•×ª ×œ-Enhanced TelemetryGeneratorPro
"""
from typing import List, Dict, Any

# from ast import Dict, List
import os
import json
import asyncio
import random
import time
from pathlib import Path

from pyparsing import Any
from generator import EnhancedBitPacker,Enum,EnhancedTelemetryGeneratorPro,GPUAcceleratedGenerator, OutputFormat, RecordType
# from generator import *

def create_sample_schemas():
    """×™×•×¦×¨ schemas ×œ×“×•×’×× ×œ××§×¨×™ ×©×™××•×© ×©×•× ×™×"""
    schemas = {}
    
    # 1. IoT Sensors - ×—×™×™×©× ×™× ×¤×©×•×˜×™×
    schemas["iot_sensors"] = {
        "temperature": {"type": "float", "bits": 32, "min": -50.0, "max": 150.0, "encoding": "ieee754"},
        "humidity": {"type": "float", "bits": 32, "min": 0.0, "max": 100.0},
        "pressure": {"type": "int", "bits": 16, "min": 800, "max": 1200},
        "battery_level": {"type": "int", "bits": 8},
        "sensor_id": {"type": "string", "length": 8, "compressed": True, "char_bits": 6},
        "is_active": {"type": "bool"},
        "last_seen": {"type": "time", "bits": 32, "range": 86400}  # 24 hours
    }
    
    # 2. Server Monitoring - × ×™×˜×•×¨ ×©×¨×ª×™×
    schemas["server_monitoring"] = {
        "cpu_usage": {"type": "int", "bits": 8},  # 0-100%
        "memory_usage": {"type": "int", "bits": 8},  # 0-100%
        "disk_usage": {"type": "int", "bits": 8},  # 0-100%
        "network_in": {"type": "int", "bits": 32},  # bytes/sec
        "network_out": {"type": "int", "bits": 32},  # bytes/sec
        "active_connections": {"type": "int", "bits": 16},
        "server_status": {"type": "enum", "values": [0, 1, 2, 3, 4]},  # ok, warning, error, critical, unknown
        "hostname": {"type": "string", "length": 16, "compressed": True},
        "uptime": {"type": "time", "bits": 32, "range": 2592000},  # 30 days
        "load_average": {"type": "float", "bits": 32, "min": 0.0, "max": 100.0}
    }
    
    # 3. Financial Trading - ××¡×—×¨ ×¤×™× × ×¡×™
    schemas["trading_data"] = {
        "symbol": {"type": "string", "length": 6, "compressed": True},  # AAPL, MSFT, etc.
        "price": {"type": "float", "bits": 32, "min": 0.01, "max": 10000.0, "encoding": "ieee754"},
        "volume": {"type": "int", "bits": 32},
        "bid_price": {"type": "float", "bits": 32, "encoding": "ieee754"},
        "ask_price": {"type": "float", "bits": 32, "encoding": "ieee754"},
        "market_status": {"type": "enum", "values": [0, 1, 2, 3]},  # pre_market, open, closed, after_hours
        "exchange": {"type": "enum", "values": [0, 1, 2, 3, 4]},  # NYSE, NASDAQ, etc.
        "trade_timestamp": {"type": "time", "bits": 32, "range": 86400}
    }
    
    # 4. Vehicle Telemetry - ×˜×œ××˜×¨×™×™×ª ×¨×›×‘×™×
    schemas["vehicle_telemetry"] = {
        "vehicle_id": {"type": "string", "length": 12, "compressed": True},
        "speed": {"type": "int", "bits": 8},  # km/h 0-255
        "engine_temp": {"type": "int", "bits": 8, "min": 60, "max": 120},  # Celsius
        "fuel_level": {"type": "int", "bits": 8},  # 0-100%
        "latitude": {"type": "float", "bits": 32, "encoding": "ieee754"},
        "longitude": {"type": "float", "bits": 32, "encoding": "ieee754"},
        "engine_status": {"type": "enum", "values": [0, 1, 2, 3]},  # off, idle, running, error
        "gear": {"type": "enum", "values": [0, 1, 2, 3, 4, 5, 6, 7]},  # neutral, 1-6, reverse
        "odometer": {"type": "int", "bits": 24},  # km
        "door_status": {"type": "int", "bits": 4}  # bitmask for 4 doors
    }
    
    return schemas

def example_basic_usage():
    """×“×•×’×× ×‘×¡×™×¡×™×ª ×œ×©×™××•×©"""
    print("ğŸš€ ×“×•×’×× 1: ×©×™××•×© ×‘×¡×™×¡×™")
    
    # ×™×¦×™×¨×ª schema
    schemas = create_sample_schemas()
    schema_file = "iot_sensors_schema.json"
    
    with open(schema_file, 'w') as f:
        json.dump(schemas["iot_sensors"], f, indent=2)
    
    try:
        # ×™×¦×™×¨×ª ××—×•×œ×œ
        generator = EnhancedTelemetryGeneratorPro(
            schema_file,
            output_dir="./output",
            default_record_type=RecordType.UPDATE
        )
        
        # ××™×“×¢ ×¢×œ ×”×¡×›××”
        info = generator.get_enhanced_schema_info()
        print(f"ğŸ“Š {info['data_fields_count']} ×©×“×•×ª, {info['bytes_per_record']} bytes ×œ×¨×©×•××”")
        
        # ×™×¦×™×¨×ª ×§×•×‘×¥ ×™×—×™×“
        print("ğŸ“ ×™×•×¦×¨ ×§×•×‘×¥ JSON ×œ×“×™×‘×•×’×™× ×’...")
        generator.write_records_enhanced(
            "./output/iot_debug.json",
            100,  # 100 records
            output_format=OutputFormat.JSON
        )
        
        # ×™×¦×™×¨×ª ×§×•×‘×¥ ×‘×™× ××¨×™ ×œ×¤×¨×•×“×§×¦×™×”
        print("âš¡ ×™×•×¦×¨ ×§×•×‘×¥ ×‘×™× ××¨×™ ××”×™×¨...")
        generator.write_records_enhanced(
            "./output/iot_production.bin",
            10000,  # 10K records
            output_format=OutputFormat.BINARY
        )
        
        print("âœ… ×”×¡×ª×™×™× ×‘×”×¦×œ×—×”!")
        
    finally:
        # × ×™×§×•×™
        if os.path.exists(schema_file):
            os.remove(schema_file)

def example_multiple_formats():
    """×“×•×’×× ×œ×™×¦×™×¨×ª ××¡×¤×¨ ×¤×•×¨××˜×™×"""
    print("\nğŸ¯ ×“×•×’×× 2: ×¤×•×¨××˜×™× ××¨×•×‘×™×")
    
    schemas = create_sample_schemas()
    schema_file = "server_monitoring_schema.json"
    
    with open(schema_file, 'w') as f:
        json.dump(schemas["server_monitoring"], f, indent=2)
    
    try:
        generator = EnhancedTelemetryGeneratorPro(
            schema_file,
            output_dir="./output/monitoring"
        )
        
        formats_config = [
            {
                "format": OutputFormat.BINARY,
                "records": 50000,
                "prefix": "monitoring_binary",
                "description": "×”×¤×¨×•×“×§×¦×™×” ×”××”×™×¨×”"
            },
            {
                "format": OutputFormat.INFLUX_LINE,
                "records": 10000,
                "prefix": "monitoring_influx",
                "description": "×™×‘×•× ×œ-InfluxDB"
            },
            {
                "format": OutputFormat.JSON,
                "records": 1000,
                "prefix": "monitoring_debug",
                "description": "×“×™×‘×•×’×™× ×’ ×•×¤×™×ª×•×—"
            }
        ]
        
        for config in formats_config:
            print(f"ğŸ“„ ×™×•×¦×¨ {config['description']} - {config['records']:,} records...")
            
            # ×”×¢×¨×›×ª ×’×•×“×œ
            storage = generator.estimate_storage_requirements(
                config["records"], 
                config["format"],
                compression_ratio=0.65
            )
            
            print(f"   ğŸ’¾ ×’×•×“×œ ×¦×¤×•×™: {storage['compressed_mb']:.1f} MB")
            
            # ×™×¦×™×¨×” ×‘×¤×•×¢×œ
            start_time = time.time()
            
            paths = generator.generate_multiple_files_enhanced(
                num_files=3,
                records_per_file=config["records"] // 3,
                file_prefix=config["prefix"],
                output_format=config["format"],
                max_workers=4,
                record_type_ratio={
                    RecordType.UPDATE: 0.8,  # 80% updates
                    RecordType.EVENT: 0.2    # 20% events
                }
            )
            
            elapsed = time.time() - start_time
            total_size = sum(os.path.getsize(path) for path in paths)
            actual_mb = total_size / (1024 * 1024)
            
            print(f"   âœ… {len(paths)} ×§×‘×¦×™×, {actual_mb:.1f} MB, {elapsed:.1f}s")
            print(f"   âš¡ {config['records']/elapsed:.0f} records/sec")
        
    finally:
        if os.path.exists(schema_file):
            os.remove(schema_file)

def example_high_performance():
    """×“×•×’×× ×œ×‘×™×¦×•×¢×™× ×’×‘×•×”×™× ×¢× GPU"""
    print("\nâš¡ ×“×•×’×× 3: ×‘×™×¦×•×¢×™× ×’×‘×•×”×™×")
    
    schemas = create_sample_schemas()
    schema_file = "trading_schema.json"
    
    with open(schema_file, 'w') as f:
        json.dump(schemas["trading_data"], f, indent=2)
    
    try:
        generator = EnhancedTelemetryGeneratorPro(
            schema_file,
            output_dir="./output/trading",
            enable_gpu=True  # ×™× ×¡×” ×œ×”×©×ª××© ×‘GPU
        )
        
        # ×‘×“×™×§×ª ×™×›×•×œ×•×ª GPU
        info = generator.get_enhanced_schema_info()
        print(f"ğŸ”§ GPU ×–××™×Ÿ: {info['supports_gpu']}")
        
        # benchmark
        print("ğŸƒ ××¨×™×¥ benchmark...")
        results = generator.benchmark_generation_speed(
            test_records=10000,
            use_gpu=True,
            batch_size=2000
        )
        
        for metric, value in results.items():
            print(f"   {metric}: {value:.1f}")
        
        # ×™×¦×™×¨×” ×¢× batches ×’×“×•×œ×™×
        print("ğŸš€ ×™×•×¦×¨ × ×ª×•× ×™× ×‘volume ×’×‘×•×”...")
        
        start_time = time.time()
        paths = generator.generate_multiple_files_enhanced(
            num_files=10,  # 10 ×§×‘×¦×™×
            records_per_file=100000,  # 100K records ×›×œ ××—×“ = 1M total
            file_prefix="trading_high_volume",
            output_format=OutputFormat.BINARY,
            max_workers=8,
            use_gpu_batches=True,
            batch_size=5000,
            record_type_ratio={
                RecordType.UPDATE: 0.9,  # ×¨×•×‘ ×¢×“×›×•× ×™ ××—×™×¨×™×
                RecordType.EVENT: 0.1    # ××¢×˜ ××™×¨×•×¢×™ ××¡×—×¨ ××™×•×—×“×™×
            }
        )
        elapsed = time.time() - start_time
        
        total_records = 10 * 100000
        total_size = sum(os.path.getsize(path) for path in paths)
        
        print(f"âœ… ×™×¦×¨ {total_records:,} ×¨×©×•××•×ª ×‘××”×™×¨×•×ª!")
        print(f"   ğŸ“ {len(paths)} ×§×‘×¦×™×")
        print(f"   ğŸ’¾ {total_size / (1024**3):.2f} GB")
        print(f"   â±ï¸ {elapsed:.1f} ×©× ×™×•×ª")
        print(f"   âš¡ {total_records/elapsed:,.0f} records/sec")
        
    finally:
        if os.path.exists(schema_file):
            os.remove(schema_file)

async def example_async_pipeline():
    """×“×•×’×× ×œ-pipeline ××¡×™× ×›×¨×•× ×™"""
    print("\nğŸ”„ ×“×•×’×× 4: Pipeline ××¡×™× ×›×¨×•× ×™")
    
    schemas = create_sample_schemas()
    schema_file = "vehicle_schema.json"
    
    with open(schema_file, 'w') as f:
        json.dump(schemas["vehicle_telemetry"], f, indent=2)
    
    try:
        generator = EnhancedTelemetryGeneratorPro(
            schema_file,
            output_dir="./output/vehicles"
        )
        
        print("ğŸš— ×™×•×¦×¨ × ×ª×•× ×™ ×¨×›×‘×™× ×‘××§×‘×™×œ...")
        
        # ×™×¦×™×¨×” ××¡×™× ×›×¨×•× ×™×ª ×©×œ ××¡×¤×¨ datasets
        tasks = [
            generator.generate_multiple_files_enhanced_async(
                num_files=5,
                records_per_file=20000,
                file_prefix="fleet_morning",
                output_format=OutputFormat.INFLUX_LINE,
                record_type_ratio={RecordType.UPDATE: 0.95, RecordType.EVENT: 0.05}
            ),
            generator.generate_multiple_files_enhanced_async(
                num_files=5,
                records_per_file=20000,
                file_prefix="fleet_evening",
                output_format=OutputFormat.BINARY,
                record_type_ratio={RecordType.UPDATE: 0.85, RecordType.EVENT: 0.15}
            )
        ]
        
        # ×”××ª× ×” ×œ×›×œ ×”××©×™××•×ª
        start_time = time.time()
        results = await asyncio.gather(*tasks)
        elapsed = time.time() - start_time
        
        total_files = sum(len(paths) for paths in results)
        total_records = 5 * 20000 * 2  # 2 datasets
        
        print(f"âœ… Pipeline ××¡×™× ×›×¨×•× ×™ ×”×•×©×œ×!")
        print(f"   ğŸ“ {total_files} ×§×‘×¦×™×")
        print(f"   ğŸ“Š {total_records:,} ×¨×©×•××•×ª")
        print(f"   â±ï¸ {elapsed:.1f} ×©× ×™×•×ª")
        print(f"   âš¡ {total_records/elapsed:,.0f} records/sec")
        
    finally:
        if os.path.exists(schema_file):
            os.remove(schema_file)

def example_influxdb_integration():
    """×“×•×’×× ×œ××™× ×˜×’×¨×¦×™×” ×¢× InfluxDB"""
    print("\nğŸ“ˆ ×“×•×’×× 5: ××™× ×˜×’×¨×¦×™×” ×¢× InfluxDB")
    
    schemas = create_sample_schemas()
    schema_file = "mixed_telemetry_schema.json"
    
    # schema ××¢×•×¨×‘×ª ×œ×“×•×’××
    mixed_schema = {
        **schemas["iot_sensors"],
        **{k: v for k, v in schemas["server_monitoring"].items() 
           if k not in schemas["iot_sensors"]}
    }
    
    with open(schema_file, 'w') as f:
        json.dump(mixed_schema, f, indent=2)
    
    try:
        generator = EnhancedTelemetryGeneratorPro(
            schema_file,
            output_dir="./output/influxdb_import"
        )
        
        # ×™×¦×™×¨×ª × ×ª×•× ×™× ×‘×¤×•×¨××˜ InfluxDB
        print("ğŸ“Š ×™×•×¦×¨ × ×ª×•× ×™× ×œimport ×œ-InfluxDB...")
        
        influx_paths = generator.generate_multiple_files_enhanced(
            num_files=3,
            records_per_file=5000,
            file_prefix="influx_data",
            output_format=OutputFormat.INFLUX_LINE,
            record_type_ratio={
                RecordType.UPDATE: 0.7,
                RecordType.EVENT: 0.3
            }
        )
        
        print(f"âœ… ×™×¦×¨ {len(influx_paths)} ×§×‘×¦×™× ×œInfluxDB")
        
        # ×”×¦×’×ª ×“×•×’×× ×©×œ Line Protocol
        print("\nğŸ“„ ×“×•×’×× ××§×•×‘×¥ InfluxDB:")
        with open(influx_paths[0], 'r') as f:
            for i, line in enumerate(f):
                if i < 3:  # ×”×¦×’ 3 ×©×•×¨×•×ª ×¨××©×•× ×•×ª
                    print(f"   {line.strip()}")
                else:
                    break
        
        # ×”×•×¨××•×ª import
        print(f"\nğŸ’¡ ×œ×™×™×‘× ×œ-InfluxDB:")
        print(f"   influx write -b your-bucket -o your-org -f {influx_paths[0]}")
        
    finally:
        if os.path.exists(schema_file):
            os.remove(schema_file)

def example_performance_analysis():
    """×“×•×’×× ×œ× ×™×ª×•×— ×‘×™×¦×•×¢×™× ××¤×•×¨×˜"""
    print("\nğŸ“Š ×“×•×’×× 6: × ×™×ª×•×— ×‘×™×¦×•×¢×™×")
    
    schemas = create_sample_schemas()
    test_configs = [
        ("iot_sensors", "IoT Sensors", 1000),
        ("server_monitoring", "Server Monitoring", 2000),
        ("trading_data", "Trading Data", 5000)
    ]
    
    results = {}
    
    for schema_name, display_name, num_records in test_configs:
        print(f"\nğŸ”¬ ×‘×•×“×§ {display_name}...")
        
        schema_file = f"{schema_name}_perf.json"
        with open(schema_file, 'w') as f:
            json.dump(schemas[schema_name], f)
        
        try:
            generator = EnhancedTelemetryGeneratorPro(
                schema_file,
                output_dir="./output/performance"
            )
            
            # ××™×“×¢ ×¢×œ ×¡×›××”
            info = generator.get_enhanced_schema_info()
            
            # benchmark
            bench_results = generator.benchmark_generation_speed(
                test_records=num_records,
                use_gpu=True,
                batch_size=min(500, num_records // 4)
            )
            
            # ×”×¢×¨×›×ª ××—×¡×•×Ÿ
            storage_binary = generator.estimate_storage_requirements(
                num_records, OutputFormat.BINARY, compression_ratio=0.6
            )
            storage_json = generator.estimate_storage_requirements(
                num_records, OutputFormat.JSON, compression_ratio=0.4
            )
            
            results[schema_name] = {
                "fields": info["data_fields_count"],
                "bytes_per_record": info["bytes_per_record"],
                "records_per_sec": bench_results["regular_records_per_sec"],
                "binary_mb": storage_binary["compressed_mb"],
                "json_mb": storage_json["compressed_mb"],
                "compression_ratio": storage_json["compressed_mb"] / storage_binary["compressed_mb"]
            }
            
            print(f"   ğŸ“ {info['data_fields_count']} ×©×“×•×ª, {info['bytes_per_record']} bytes/record")
            print(f"   âš¡ {bench_results['regular_records_per_sec']:,.0f} records/sec")
            print(f"   ğŸ’¾ Binary: {storage_binary['compressed_mb']:.1f} MB")
            print(f"   ğŸ“„ JSON: {storage_json['compressed_mb']:.1f} MB (×¤×™ {storage_json['compressed_mb']/storage_binary['compressed_mb']:.1f})")
            
        finally:
            if os.path.exists(schema_file):
                os.remove(schema_file)
    
    # ×¡×™×›×•× ×”×©×•×•××ª×™
    print(f"\nğŸ“‹ ×¡×™×›×•× ×‘×™×¦×•×¢×™×:")
    print(f"{'Schema':<20} {'Fields':<8} {'Rec/Sec':<10} {'Binary MB':<12} {'JSON Ratio':<12}")
    print("-" * 70)
    for name, data in results.items():
        print(f"{name:<20} {data['fields']:<8} {data['records_per_sec']:<10.0f} "
              f"{data['binary_mb']:<12.1f} {data['compression_ratio']:<12.1f}")

def example_production_simulation():
    """×“×•×’×× ×œ×¡×™××•×œ×¦×™×™×ª ×¤×¨×•×“×§×¦×™×” ××œ××”"""
    print("\nğŸ­ ×“×•×’×× 7: ×¡×™××•×œ×¦×™×™×ª ×¤×¨×•×“×§×¦×™×”")
    
    schemas = create_sample_schemas()
    
    # ×ª×¨×—×™×©: ×—×‘×¨×ª logistics ×¢× ×¦×™ ×¨×›×‘×™×
    scenario_config = {
        "num_vehicles": 1000,
        "records_per_vehicle_per_hour": 360,  # ×›×œ 10 ×©× ×™×•×ª
        "simulation_hours": 24,
        "files_per_hour": 12,  # ×›×œ 5 ×“×§×•×ª ×§×•×‘×¥ ×—×“×©
        "output_formats": [OutputFormat.BINARY, OutputFormat.INFLUX_LINE]
    }
    
    total_records = (scenario_config["num_vehicles"] * 
                    scenario_config["records_per_vehicle_per_hour"] * 
                    scenario_config["simulation_hours"])
    
    print(f"ğŸš› ××“××” {scenario_config['num_vehicles']} ×¨×›×‘×™× ×œ××©×š {scenario_config['simulation_hours']} ×©×¢×•×ª")
    print(f"ğŸ“Š ×¡×”×´×› {total_records:,} ×¨×©×•××•×ª ×¦×¤×•×™×•×ª")
    
    schema_file = "production_vehicle_schema.json"
    with open(schema_file, 'w') as f:
        json.dump(schemas["vehicle_telemetry"], f)
    
    try:
        generator = EnhancedTelemetryGeneratorPro(
            schema_file,
            output_dir="./output/production_simulation",
            enable_gpu=True
        )
        
        # ×”×¢×¨×›×ª ×“×¨×™×©×•×ª
        estimation = generator.estimate_storage_requirements(
            total_records,
            OutputFormat.BINARY,
            compression_ratio=0.5  # ×“×—×™×¡×” ×—×–×§×” ×œ×¤×¨×•×“×§×¦×™×”
        )
        
        print(f"ğŸ’¾ ×¦×¤×•×™: {estimation['compressed_gb']:.1f} GB ××—×¨×™ ×“×—×™×¡×”")
        
        # ×™×¦×™×¨×ª ×”× ×ª×•× ×™× ×‘×©× ×™ ×¤×•×¨××˜×™×
        all_paths = []
        
        for output_format in scenario_config["output_formats"]:
            print(f"\nğŸ“ ×™×•×¦×¨ ×‘×¤×•×¨××˜ {output_format.value}...")
            
            records_per_file = total_records // scenario_config["files_per_hour"] // scenario_config["simulation_hours"]
            total_files = scenario_config["files_per_hour"] * scenario_config["simulation_hours"]
            
            start_time = time.time()
            
            paths = generator.generate_multiple_files_enhanced(
                num_files=total_files,
                records_per_file=records_per_file,
                file_prefix=f"logistics_{output_format.value}",
                output_format=output_format,
                max_workers=8,
                use_gpu_batches=True,
                batch_size=10000,
                record_type_ratio={
                    RecordType.UPDATE: 0.85,  # ××™×§×•×/××”×™×¨×•×ª ×¨×’×™×œ×™×
                    RecordType.EVENT: 0.15    # ×¢×¦×™×¨×•×ª, ×ª×“×œ×•×§, ××™×¨×•×¢×™×
                }
            )
            
            elapsed = time.time() - start_time
            total_size = sum(os.path.getsize(path) for path in paths)
            
            print(f"   âœ… {len(paths)} ×§×‘×¦×™×, {total_size/(1024**3):.2f} GB")
            print(f"   â±ï¸ {elapsed:.1f} ×©× ×™×•×ª")
            print(f"   âš¡ {total_records/elapsed:,.0f} records/sec")
            
            all_paths.extend(paths)
        
        print(f"\nğŸ¯ ×¡×™××•×œ×¦×™×™×ª ×¤×¨×•×“×§×¦×™×” ×”×•×©×œ××”!")
        print(f"   ğŸ“ ×¡×”×´×› {len(all_paths)} ×§×‘×¦×™×")
        print(f"   ğŸ“Š {total_records:,} ×¨×©×•××•×ª")
        
    finally:
        if os.path.exists(schema_file):
            os.remove(schema_file)

def example_custom_schema_creation():
    """×“×•×’×× ×œ×™×¦×™×¨×ª schema ××•×ª×× ××™×©×™×ª"""
    print("\nğŸ› ï¸ ×“×•×’×× 8: ×™×¦×™×¨×ª Schema ××•×ª××")
    
    # ×¤×•× ×§×¦×™×” ×œ×™×¦×™×¨×ª schema ×“×™× ×××™×ª
    def create_custom_schema(device_types: List[str], metrics_per_device: int) -> Dict[str, Any]:
        schema = {}
        
        # ×©×“×•×ª ×‘×¡×™×¡×™×™×
        schema["device_id"] = {"type": "string", "length": 12, "compressed": True}
        schema["timestamp"] = {"type": "time", "bits": 32, "range": 86400}
        schema["device_type"] = {"type": "enum", "values": list(range(len(device_types)))}
        
        # ××˜×¨×™×§×•×ª ×“×™× ×××™×•×ª
        for i in range(metrics_per_device):
            schema[f"metric_{i}"] = {
                "type": "float" if i % 2 == 0 else "int",
                "bits": 32 if i % 2 == 0 else random.choice([8, 16, 24]),
                "min": 0.0 if i % 2 == 0 else 0,
                "max": 100.0 if i % 2 == 0 else (1 << 16) - 1
            }
        
        # ××˜×-×“×˜×”
        schema["quality_score"] = {"type": "int", "bits": 4}  # 0-15
        schema["is_validated"] = {"type": "bool"}
        schema["error_flags"] = {"type": "int", "bits": 8}  # bitmask
        
        return schema
    
    # ×™×¦×™×¨×ª schema ××•×ª×××ª
    device_types = ["sensor", "gateway", "controller", "actuator"]
    custom_schema = create_custom_schema(device_types, 8)
    
    schema_file = "custom_schema.json"
    with open(schema_file, 'w') as f:
        json.dump(custom_schema, f, indent=2)
    
    print(f"ğŸ”§ ×™×¦×¨ schema ×¢× {len(custom_schema)} ×©×“×•×ª")
    
    try:
        generator = EnhancedTelemetryGeneratorPro(
            schema_file,
            output_dir="./output/custom"
        )
        
        # × ×™×ª×•×— ×”soma ×©× ×•×¦×¨×”
        info = generator.get_enhanced_schema_info()
        print(f"ğŸ“Š Schema analysis:")
        print(f"   ğŸ”¢ {info['data_fields_count']} data fields")
        print(f"   ğŸ’¾ {info['bytes_per_record']} bytes per record")
        print(f"   ğŸ”§ {info['overhead_bits']} overhead bits")
        
        # ×™×¦×™×¨×ª ×“×•×’×××•×ª ×‘×›×œ ×”×¤×•×¨××˜×™×
        for format_type in OutputFormat:
            format_name = format_type.value
            extension = {"binary": ".bin", "json": ".json", "influx_line": ".txt"}[format_name]
            
            print(f"\nğŸ“ ×™×•×¦×¨ ×“×•×’×× ×‘{format_name}...")
            
            paths = generator.generate_multiple_files_enhanced(
                num_files=2,
                records_per_file=1000,
                file_prefix=f"custom_{format_name}",
                output_format=format_type
            )
            
            total_size = sum(os.path.getsize(path) for path in paths)
            print(f"   âœ… {len(paths)} ×§×‘×¦×™×, {total_size/1024:.1f} KB")
            
            # ×”×¦×’×ª ×“×•×’×× ××”×§×•×‘×¥ ×”×¨××©×•×Ÿ (×œ× ×‘×™× ××¨×™)
            if format_type != OutputFormat.BINARY:
                with open(paths[0], 'r') as f:
                    first_line = f.readline().strip()
                    print(f"   ğŸ“„ ×“×•×’××: {first_line[:100]}...")
        
    finally:
        if os.path.exists(schema_file):
            os.remove(schema_file)

def example_monitoring_and_validation():
    """×“×•×’×× ×œ× ×™×˜×•×¨ ×•×•×•×œ×™×“×¦×™×”"""
    print("\nğŸ” ×“×•×’×× 9: × ×™×˜×•×¨ ×•×•×•×œ×™×“×¦×™×”")
    
    # ×™×¦×™×¨×ª schema ×¢× ×•×•×œ×™×“×¦×™×•×ª
    validation_schema = {
        "cpu_temp": {"type": "float", "bits": 32, "min": 20.0, "max": 90.0},
        "fan_speed": {"type": "int", "bits": 12, "min": 0, "max": 4000},  # RPM
        "voltage": {"type": "float", "bits": 32, "min": 11.5, "max": 12.5},
        "current": {"type": "float", "bits": 32, "min": 0.0, "max": 20.0},
        "power_state": {"type": "enum", "values": [0, 1, 2, 3, 4]},
        "alert_level": {"type": "enum", "values": [0, 1, 2, 3]},  # none, info, warning, critical
        "node_id": {"type": "string", "length": 6, "compressed": True}
    }
    
    schema_file = "validation_schema.json"
    with open(schema_file, 'w') as f:
        json.dump(validation_schema, f, indent=2)
    
    try:
        generator = EnhancedTelemetryGeneratorPro(
            schema_file,
            output_dir="./output/monitoring"
        )
        
        print("ğŸ” ×™×•×¦×¨ × ×ª×•× ×™× ×¢× × ×™×˜×•×¨...")
        
        def progress_monitor(current, total):
            """Callback ×œ× ×™×˜×•×¨ ×”×ª×§×“××•×ª"""
            percent = (current / total) * 100
            if percent % 20 == 0:  # ×›×œ 20%
                print(f"   ğŸ“ˆ ×”×ª×§×“××•×ª: {percent:.0f}% ({current:,}/{total:,} records)")
        
        # ×™×¦×™×¨×” ×¢× × ×™×˜×•×¨ ×”×ª×§×“××•×ª
        start_time = time.time()
        
        paths = generator.generate_multiple_files_enhanced(
            num_files=4,
            records_per_file=25000,  # 100K total
            file_prefix="monitored_data",
            output_format=OutputFormat.BINARY,
            max_workers=4
        )
        
        elapsed = time.time() - start_time
        
        print(f"âœ… × ×™×˜×•×¨ ×”×•×©×œ×!")
        print(f"   ğŸ“ {len(paths)} ×§×‘×¦×™×")
        print(f"   â±ï¸ {elapsed:.1f} ×©× ×™×•×ª")
        
        # ×‘×“×™×§×ª ×©×œ××•×ª ×”× ×ª×•× ×™×
        print("\nğŸ” ×‘×•×“×§ ×©×œ××•×ª × ×ª×•× ×™×...")
        
        total_size = 0
        for i, path in enumerate(paths):
            file_size = os.path.getsize(path)
            total_size += file_size
            print(f"   ğŸ“„ ×§×•×‘×¥ {i}: {file_size:,} bytes")
        
        expected_size = 100000 * generator.get_enhanced_schema_info()["bytes_per_record"]
        size_ratio = total_size / expected_size
        
        print(f"   ğŸ“Š ×¡×”×´×›: {total_size:,} bytes")
        print(f"   ğŸ¯ ×™×—×¡ ×œ×¦×¤×•×™: {size_ratio:.2f} ({'âœ… ×ª×§×™×Ÿ' if 0.95 <= size_ratio <= 1.05 else 'âš ï¸ ×—×¨×™×’'})")
        
    finally:
        if os.path.exists(schema_file):
            os.remove(schema_file)

def main():
    os.makedirs('./output', exist_ok=True)
    os.makedirs('./output/monitoring', exist_ok=True)
    os.makedirs('./output/influxdb_import', exist_ok=True)
    os.makedirs('./output/trading', exist_ok=True)
    os.makedirs('./output/vehicles', exist_ok=True)
    os.makedirs('./output/performance', exist_ok=True)
    os.makedirs('./output/custom', exist_ok=True)

    """××¨×™×¥ ××ª ×›×œ ×”×“×•×’×××•×ª"""
    print("ğŸ¬ ××ª×—×™×œ ×“×•×’×××•×ª ×©×™××•×© ××¢×©×™×•×ª...")
    print("=" * 70)
    
    # ×™×¦×™×¨×ª ×ª×™×§×™×™×ª output
    os.makedirs("./output", exist_ok=True)
    
    try:
        # ×“×•×’×××•×ª sync
        example_basic_usage()
        example_multiple_formats() 
        example_high_performance()
        example_influxdb_integration()
        example_performance_analysis()
        example_custom_schema_creation()
        example_monitoring_and_validation()
        
        # ×“×•×’×× async
        print("\nğŸ”„ ××¨×™×¥ ×“×•×’×× ××¡×™× ×›×¨×•× ×™×ª...")
        asyncio.run(example_async_pipeline())
        
        print("\n" + "=" * 70)
        print("ğŸ‰ ×›×œ ×”×“×•×’×××•×ª ×”×•×©×œ××• ×‘×”×¦×œ×—×”!")
        print("ğŸ’¡ ×”×§×•×“ ××•×›×Ÿ ×œ×©×™××•×© ×‘×¤×¨×•×“×§×¦×™×”!")
        
        # ×”×¦×’×ª ×¡×™×›×•× ×§×‘×¦×™× ×©× ×•×¦×¨×•
        if os.path.exists("./output"):
            total_files = sum(len(files) for _, _, files in os.walk("./output"))
            total_size = sum(
                os.path.getsize(os.path.join(root, file))
                for root, _, files in os.walk("./output")
                for file in files
            )
            
            print(f"\nğŸ“ × ×•×¦×¨×• {total_files} ×§×‘×¦×™×")
            print(f"ğŸ’¾ ×¡×”×´×› ×’×•×“×œ: {total_size/(1024**2):.1f} MB")
    
    except Exception as e:
        print(f"\nâŒ ×©×’×™××”: {e}")
        raise


if __name__ == "__main__":
    main()