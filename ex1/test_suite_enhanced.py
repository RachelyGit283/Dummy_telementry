# import os
# import json
# import tempfile
# import unittest
# import shutil
# import struct
# import time
# from typing import Dict, Any
# import logging
# from generator  import EnhancedBitPacker,EnhancedTelemetryGeneratorPro,Enum,GPUAcceleratedGenerator,InfluxDBLineProtocolWriter
# # Imports ×©×œ ×”×”×¨×—×‘×•×ª
# # from generator import *

# class TestEnhancedBitPacker(unittest.TestCase):
#     """×‘×“×™×§×•×ª ×œ-EnhancedBitPacker"""
    
#     def setUp(self):
#         self.packer = EnhancedBitPacker()
    
#     def test_float_packing(self):
#         """×‘×“×™×§×ª ××¨×™×–×ª float"""
#         test_value = 3.14159
#         self.packer.write_float(test_value, precision_bits=16)
#         self.packer.flush_to_byte_boundary()
#         result = self.packer.bytes()
        
#         # ×‘×“×™×§×” ×©×”×ª×•×¦××” ××™× × ×” ×¨×™×§×”
#         self.assertGreater(len(result), 0)
#         self.assertEqual(len(result), 4)  # 32 bits = 4 bytes

#     def test_ieee754_float(self):
#         """×‘×“×™×§×ª IEEE 754 float encoding"""
#         test_value = 42.5
#         self.packer.write_ieee754_float(test_value)
#         result = self.packer.bytes()
        
#         # ×”×©×•×•××” ×¢× struct.pack
#         expected = struct.pack('>f', test_value)
#         self.assertEqual(result, expected)

#     def test_string_packing(self):
#         """×‘×“×™×§×ª ××¨×™×–×ª string"""
#         test_string = "Hello"
#         self.packer.write_string(test_string)
#         self.packer.flush_to_byte_boundary()
#         result = self.packer.bytes()
        
#         # ×‘×“×™×§×”: length byte + string bytes
#         expected_length = len(test_string)
#         self.assertEqual(result[0], expected_length)
#         self.assertEqual(result[1:expected_length+1], test_string.encode('ascii'))

#     def test_compressed_string(self):
#         """×‘×“×™×§×ª string ×“×—×•×¡"""
#         test_string = "ABC123"
#         self.packer.write_compressed_string(test_string, char_bits=6)
#         self.packer.flush_to_byte_boundary()
#         result = self.packer.bytes()
        
#         # ×‘×“×™×§×” ×©×™×© × ×ª×•× ×™×
#         self.assertGreater(len(result), 1)  # ×œ×¤×—×•×ª length + data


# class TestInfluxDBLineProtocol(unittest.TestCase):
#     """×‘×“×™×§×•×ª ×œ×¤×•×¨××˜ InfluxDB Line Protocol"""
    
#     def test_basic_line_format(self):
#         """×‘×“×™×§×ª ×¤×•×¨××˜ ×‘×¡×™×¡×™"""
#         measurement = "temperature"
#         tags = {"sensor": "A", "location": "room1"}
#         fields = {"value": 23.5, "status": True}
#         timestamp = 1640995200000000000
        
#         line = InfluxDBLineProtocolWriter.format_record(
#             measurement, tags, fields, timestamp
#         )
        
#         # ×‘×“×™×§×” ×©×”×¤×•×¨××˜ × ×›×•×Ÿ
#         self.assertIn("temperature,sensor=A,location=room1", line)
#         self.assertIn('value=23.5,status=false', line)
#         self.assertIn(str(timestamp), line)

#     def test_string_field_quoting(self):
#         """×‘×“×™×§×ª ×¦×™×˜×•×˜ ×©×œ string fields"""
#         measurement = "logs"
#         tags = {}
#         fields = {"message": "Hello World", "level": "info"}
#         timestamp = 0
        
#         line = InfluxDBLineProtocolWriter.format_record(
#             measurement, tags, fields, timestamp
#         )
        
#         self.assertIn('"Hello World"', line)
#         self.assertIn('"info"', line)

#     def test_integer_field_suffix(self):
#         """×‘×“×™×§×ª ×¡×™×•××ª i ×œ×©×“×•×ª int"""
#         measurement = "counter"
#         tags = {}
#         fields = {"count": 42}
#         timestamp = 0
        
#         line = InfluxDBLineProtocolWriter.format_record(
#             measurement, tags, fields, timestamp
#         )
        
#         self.assertIn('count=42i', line)


# class TestEnhancedTelemetryGenerator(unittest.TestCase):
#     """×‘×“×™×§×•×ª ××ª×§×“××•×ª ×œ××—×œ×§×ª ×”×¨××©×™×ª"""
    
#     def setUp(self):
#         self.temp_dir = tempfile.mkdtemp()
#         self.schema_file = os.path.join(self.temp_dir, "enhanced_schema.json")
        
#         # Schema ××ª×§×“××ª
#         self.enhanced_schema = {
#             "temperature": {"type": "float", "bits": 32, "min": -50.0, "max": 150.0},
#             "pressure": {"type": "int", "bits": 16},
#             "device_id": {"type": "string", "length": 8, "max_length": 16},
#             "status": {"type": "bool"},
#             "mode": {"type": "enum", "values": [0, 1, 2, 3, 4]},
#             "timestamp_offset": {"type": "time", "bits": 16, "range": 3600}
#         }
        
#         with open(self.schema_file, 'w') as f:
#             json.dump(self.enhanced_schema, f)
    
#     def tearDown(self):
#         shutil.rmtree(self.temp_dir)
    
#     def test_enhanced_record_generation(self):
#         """×‘×“×™×§×ª ×™×¦×™×¨×ª ×¨×©×•××” ××ª×§×“××ª"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file, 
#             output_dir=self.temp_dir
#         )
        
#         record = generator.generate_enhanced_record()
        
#         # ×‘×“×™×§×•×ª ×‘×¡×™×¡×™×•×ª
#         self.assertIsInstance(record, TelemetryRecord)
#         self.assertIn(record.record_type, [RecordType.UPDATE, RecordType.EVENT])
#         self.assertGreater(record.timestamp, 0)
#         self.assertGreaterEqual(record.sequence_id, 0)
        
#         # ×‘×“×™×§×ª × ×ª×•× ×™×
#         self.assertIn("temperature", record.data)
#         self.assertIn("device_id", record.data)
        
#         # ×‘×“×™×§×ª ×˜×•×•×—×™×
#         temp = record.data["temperature"]
#         self.assertGreaterEqual(temp, -50.0)
#         self.assertLessEqual(temp, 150.0)
        
#         device_id = record.data["device_id"]
#         self.assertIsInstance(device_id, str)
#         self.assertLessEqual(len(device_id), 16)

#     def test_record_type_ratios(self):
#         """×‘×“×™×§×ª ×™×—×¡ ×¡×•×’×™ ×¨×©×•××•×ª"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file,
#             output_dir=self.temp_dir
#         )
        
#         # ×™×¦×™×¨×ª ×”×¨×‘×” ×¨×©×•××•×ª ×¢× ×™×—×¡ ××•×’×“×¨
#         ratio = {RecordType.UPDATE: 0.8, RecordType.EVENT: 0.2}
#         file_path = os.path.join(self.temp_dir, "ratio_test.json")
        
#         generator.write_records_enhanced(
#             file_path,
#             1000,
#             output_format=OutputFormat.JSON,
#             record_type_ratio=ratio
#         )
        
#         # ×¡×¤×™×¨×ª ×¡×•×’×™ ×¨×©×•××•×ª
#         update_count = 0
#         event_count = 0
        
#         with open(file_path, 'r') as f:
#             for line in f:
#                 data = json.loads(line.strip())
#                 if data['type'] == 'update':
#                     update_count += 1
#                 else:
#                     event_count += 1
        
#         total = update_count + event_count
#         update_ratio = update_count / total
#         event_ratio = event_count / total
        
#         # ×‘×“×™×§×” ×©×”×™×—×¡ ×§×¨×•×‘ ×œ××¦×•×¤×” (Â±10%)
#         self.assertAlmostEqual(update_ratio, 0.8, delta=0.1)
#         self.assertAlmostEqual(event_ratio, 0.2, delta=0.1)

#     def test_multiple_output_formats(self):
#         """×‘×“×™×§×ª ×¤×•×¨××˜×™ ×¤×œ×˜ ×©×•× ×™×"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file,
#             output_dir=self.temp_dir
#         )
        
#         test_cases = [
#             (OutputFormat.BINARY, ".bin"),
#             (OutputFormat.JSON, ".json"),
#             (OutputFormat.INFLUX_LINE, ".txt")
#         ]
        
#         for format_type, extension in test_cases:
#             with self.subTest(format=format_type):
#                 file_path = os.path.join(self.temp_dir, f"test{extension}")
#                 generator.write_records_enhanced(
#                     file_path,
#                     100,
#                     output_format=format_type
#                 )
                
#                 self.assertTrue(os.path.exists(file_path))
#                 self.assertGreater(os.path.getsize(file_path), 0)

#     def test_enhanced_parallel_generation(self):
#         """×‘×“×™×§×ª ×™×¦×™×¨×” ××§×‘×™×œ×™×ª ××ª×§×“××ª"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file,
#             output_dir=self.temp_dir
#         )
        
#         paths = generator.generate_multiple_files_enhanced(
#             num_files=3,
#             records_per_file=100,
#             output_format=OutputFormat.JSON,
#             record_type_ratio={RecordType.UPDATE: 0.5, RecordType.EVENT: 0.5}
#         )
        
#         self.assertEqual(len(paths), 3)
        
#         for path in paths:
#             self.assertTrue(os.path.exists(path))
#             self.assertGreater(os.path.getsize(path), 0)
            
#             # ×‘×“×™×§×” ×©×”×§×•×‘×¥ ××›×™×œ JSON ×ª×§×™×Ÿ
#             with open(path, 'r') as f:
#                 line_count = 0
#                 for line in f:
#                     data = json.loads(line.strip())
#                     self.assertIn('type', data)
#                     self.assertIn('timestamp', data)
#                     self.assertIn('data', data)
#                     line_count += 1
                
#                 self.assertEqual(line_count, 100)

#     def test_schema_info_enhanced(self):
#         """×‘×“×™×§×ª ××™×“×¢ ×¢×œ ×¡×›××” ××ª×§×“××ª"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file,
#             output_dir=self.temp_dir
#         )
        
#         info = generator.get_enhanced_schema_info()
        
#         # ×‘×“×™×§×•×ª ×‘×¡×™×¡×™×•×ª
#         self.assertIn("fields_count", info)
#         self.assertIn("overhead_bits", info)
#         self.assertIn("supported_formats", info)
#         self.assertIn("fields", info)
        
#         # ×‘×“×™×§×” ×©×›×œ ×”×¤×•×¨××˜×™× × ×ª××›×™×
#         expected_formats = ["binary", "influx_line", "json"]
#         self.assertEqual(set(info["supported_formats"]), set(expected_formats))
        
#         # ×‘×“×™×§×ª ×©×“×•×ª
#         self.assertEqual(len(info["fields"]), 6)  # 6 data fields
        
#         for field in info["fields"]:
#             self.assertIn("name", field)
#             self.assertIn("type", field)
#             self.assertIn("bits", field)

#     def test_storage_estimation(self):
#         """×‘×“×™×§×ª ×”×¢×¨×›×ª ×“×¨×™×©×•×ª ××—×¡×•×Ÿ"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file,
#             output_dir=self.temp_dir
#         )
        
#         # ×‘×“×™×§×” ×œ×¤×•×¨××˜×™× ×©×•× ×™×
#         for format_type in OutputFormat:
#             with self.subTest(format=format_type):
#                 estimation = generator.estimate_storage_requirements(
#                     10000, 
#                     format_type,
#                     compression_ratio=0.7
#                 )
                
#                 self.assertEqual(estimation["records"], 10000)
#                 self.assertEqual(estimation["format"], format_type.value)
#                 self.assertGreater(estimation["uncompressed_bytes"], 0)
#                 self.assertLess(estimation["compressed_bytes"], estimation["uncompressed_bytes"])
#                 self.assertEqual(estimation["compression_ratio"], 0.7)

#     def test_gpu_batch_generation(self):
#         """×‘×“×™×§×ª ×™×¦×™×¨×” ×‘-batch ×¢× GPU (×× ×–××™×Ÿ)"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file,
#             output_dir=self.temp_dir,
#             enable_gpu=True
#         )
        
#         batch_size = 50
#         batch = generator.generate_batch_gpu_accelerated(batch_size)
        
#         # ×‘×“×™×§×•×ª ×‘×¡×™×¡×™×•×ª
#         self.assertEqual(len(batch), batch_size)
        
#         for record in batch:
#             self.assertIsInstance(record, TelemetryRecord)
#             self.assertIn("temperature", record.data)
#             self.assertIn("device_id", record.data)

#     def test_benchmark_functionality(self):
#         """×‘×“×™×§×ª ×¤×•× ×§×¦×™×•× ×œ×™×•×ª benchmark"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file,
#             output_dir=self.temp_dir,
#             enable_gpu=True
#         )
        
#         results = generator.benchmark_generation_speed(
#             test_records=1000,
#             use_gpu=True,
#             batch_size=100
#         )
        
#         # ×‘×“×™×§×•×ª ×‘×¡×™×¡×™×•×ª
#         self.assertIn("regular_records_per_sec", results)
#         self.assertGreater(results["regular_records_per_sec"], 0)


# class TestAsyncFunctionality(unittest.TestCase):
#     """×‘×“×™×§×•×ª async"""
    
#     def setUp(self):
#         self.temp_dir = tempfile.mkdtemp()
#         self.schema_file = os.path.join(self.temp_dir, "async_schema.json")
        
#         simple_schema = {
#             "value": {"type": "int", "bits": 16},
#             "flag": {"type": "bool"}
#         }
        
#         with open(self.schema_file, 'w') as f:
#             json.dump(simple_schema, f)
    
#     def tearDown(self):
#         shutil.rmtree(self.temp_dir)
    
#     def test_async_generation(self):
#         """×‘×“×™×§×ª ×™×¦×™×¨×” ××¡×™× ×›×¨×•× ×™×ª"""
#         import asyncio
        
#         async def run_async_test():
#             generator = EnhancedTelemetryGeneratorPro(
#                 self.schema_file,
#                 output_dir=self.temp_dir
#             )
            
#             paths = await generator.generate_multiple_files_enhanced_async(
#                 num_files=2,
#                 records_per_file=50,
#                 output_format=OutputFormat.JSON
#             )
            
#             self.assertEqual(len(paths), 2)
#             for path in paths:
#                 self.assertTrue(os.path.exists(path))
        
#         # ×”×¨×¦×ª ×”×‘×“×™×§×”
#         asyncio.run(run_async_test())


# class TestPerformanceEnhanced(unittest.TestCase):
#     """×‘×“×™×§×•×ª ×‘×™×¦×•×¢×™× ××ª×§×“××•×ª"""
    
#     def setUp(self):
#         self.temp_dir = tempfile.mkdtemp()
#         self.schema_file = os.path.join(self.temp_dir, "perf_enhanced_schema.json")
        
#         # Schema ×’×“×•×œ×” ×œ×‘×“×™×§×ª ×‘×™×¦×•×¢×™×
#         self.large_schema = {}
#         for i in range(50):  # 50 ×©×“×•×ª
#             if i % 4 == 0:
#                 self.large_schema[f"float_field_{i}"] = {
#                     "type": "float", "bits": 32, "min": 0.0, "max": 100.0
#                 }
#             elif i % 4 == 1:
#                 self.large_schema[f"string_field_{i}"] = {
#                     "type": "string", "length": 10
#                 }
#             elif i % 4 == 2:
#                 self.large_schema[f"enum_field_{i}"] = {
#                     "type": "enum", "values": list(range(16))
#                 }
#             else:
#                 self.large_schema[f"int_field_{i}"] = {
#                     "type": "int", "bits": 20
#                 }
        
#         with open(self.schema_file, 'w') as f:
#             json.dump(self.large_schema, f)
    
#     def tearDown(self):
#         shutil.rmtree(self.temp_dir)
    
#     def test_large_schema_performance(self):
#         """×‘×“×™×§×ª ×‘×™×¦×•×¢×™× ×¢× ×¡×›××” ×’×“×•×œ×”"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file,
#             output_dir=self.temp_dir
#         )
        
#         start_time = time.time()
        
#         # ×™×¦×™×¨×ª ×§×•×‘×¥ ×’×“×•×œ
#         file_path = os.path.join(self.temp_dir, "large_perf_test.bin")
#         generator.write_records_enhanced(
#             file_path,
#             5000,  # 5K records
#             output_format=OutputFormat.BINARY
#         )
        
#         end_time = time.time()
#         elapsed = end_time - start_time
        
#         # ×‘×“×™×§×•×ª
#         self.assertTrue(os.path.exists(file_path))
#         self.assertGreater(os.path.getsize(file_path), 0)
        
#         records_per_sec = 5000 / elapsed
#         print(f"Large schema: {records_per_sec:.0f} records/sec")
        
#         # ×¦×¨×™×š ×œ×”×™×•×ª ×œ×¤×—×•×ª 500 records/sec
#         self.assertGreater(records_per_sec, 500)
    
#     def test_format_comparison_performance(self):
#         """×”×©×•×•××ª ×‘×™×¦×•×¢×™× ×‘×™×Ÿ ×¤×•×¨××˜×™×"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file,
#             output_dir=self.temp_dir
#         )
        
#         test_records = 1000
#         results = {}
        
#         for format_type in OutputFormat:
#             start_time = time.time()
            
#             extension = {
#                 OutputFormat.BINARY: ".bin",
#                 OutputFormat.JSON: ".json",
#                 OutputFormat.INFLUX_LINE: ".txt"
#             }[format_type]
            
#             file_path = os.path.join(self.temp_dir, f"perf_test_{format_type.value}{extension}")
#             generator.write_records_enhanced(
#                 file_path,
#                 test_records,
#                 output_format=format_type
#             )
            
#             elapsed = time.time() - start_time
#             results[format_type.value] = {
#                 "time": elapsed,
#                 "records_per_sec": test_records / elapsed,
#                 "file_size": os.path.getsize(file_path)
#             }
        
#         # ×”×“×¤×¡×ª ×ª×•×¦××•×ª
#         print("\nFormat Performance Comparison:")
#         for format_name, stats in results.items():
#             print(f"{format_name}: {stats['records_per_sec']:.0f} rec/sec, "
#                   f"size: {stats['file_size']:,} bytes")
        
#         # ×‘×“×™×§×” ×©×‘×™× ××¨×™ ×”×›×™ ××”×™×¨ ×•×§×˜×Ÿ
#         binary_stats = results["binary"]
#         json_stats = results["json"]
        
#         self.assertLess(binary_stats["file_size"], json_stats["file_size"])
    
#     def test_parallel_vs_sequential_performance(self):
#         """×”×©×•×•××ª ×‘×™×¦×•×¢×™× ××§×‘×™×œ×™ ××•×œ ×¨×¦×™×£"""
#         generator = EnhancedTelemetryGeneratorPro(
#             self.schema_file,
#             output_dir=self.temp_dir
#         )
        
#         total_records = 5000
#         num_files = 5
#         records_per_file = total_records // num_files
        
#         # ×¨×¦×™×£
#         start_time = time.time()
#         for i in range(num_files):
#             file_path = os.path.join(self.temp_dir, f"sequential_{i}.bin")
#             generator.write_records_enhanced(
#                 file_path,
#                 records_per_file,
#                 output_format=OutputFormat.BINARY
#             )
#         sequential_time = time.time() - start_time
        
#         # ××§×‘×™×œ×™
#         start_time = time.time()
#         generator.generate_multiple_files_enhanced(
#             num_files=num_files,
#             records_per_file=records_per_file,
#             file_prefix="parallel",
#             output_format=OutputFormat.BINARY,
#             max_workers=4
#         )
#         parallel_time = time.time() - start_time
        
#         print(f"\nParallel vs Sequential:")
#         print(f"Sequential: {sequential_time:.2f}s ({total_records/sequential_time:.0f} rec/sec)")
#         print(f"Parallel: {parallel_time:.2f}s ({total_records/parallel_time:.0f} rec/sec)")
#         print(f"Speedup: {sequential_time/parallel_time:.2f}x")
        
#         # ×‘×“×™×§×” ×©××§×‘×™×œ×™ ××”×™×¨ ×™×•×ª×¨ (×œ×¤×—×•×ª 10% ×©×™×¤×•×¨)
#         self.assertLess(parallel_time, sequential_time * 0.9)


# def run_comprehensive_enhanced_tests():
#     """××¨×™×¥ ××ª ×›×œ ×”×‘×“×™×§×•×ª ×”××ª×§×“××•×ª"""
#     # ×”×’×“×¨×ª logging
#     logging.basicConfig(
#         level=logging.INFO,
#         format='%(asctime)s - %(levelname)s - %(message)s'
#     )
    
#     print("ğŸš€ ××¨×™×¥ ×‘×“×™×§×•×ª ××ª×§×“××•×ª ×œ×’×¨×¡×” ×”××•×¨×—×‘×ª...")
    
#     # ×™×¦×™×¨×ª test suite
#     suite = unittest.TestSuite()
    
#     # ×”×•×¡×¤×ª ×›×œ ×§×‘×•×¦×•×ª ×”×‘×“×™×§×•×ª
#     test_classes = [
#         TestEnhancedBitPacker,
#         TestInfluxDBLineProtocol,
#         TestEnhancedTelemetryGenerator,
#         TestAsyncFunctionality,
#         TestPerformanceEnhanced
#     ]
    
#     for test_class in test_classes:
#         suite.addTest(unittest.makeSuite(test_class))
    
#     # ×”×¨×¦×ª ×”×‘×“×™×§×•×ª
#     runner = unittest.TextTestRunner(verbosity=2)
#     result = runner.run(suite)
    
#     return result.wasSuccessful()


# class IntegrationTest:
#     """×‘×“×™×§×•×ª ××™× ×˜×’×¨×¦×™×” ××œ××•×ª"""
    
#     def __init__(self, temp_dir: str):
#         self.temp_dir = temp_dir
#         self.logger = logging.getLogger(__name__)
    
#     def test_full_pipeline(self):
#         """×‘×“×™×§×ª pipeline ××œ×"""
#         print("\nğŸ”„ ××¨×™×¥ ×‘×“×™×§×ª pipeline ××œ×...")
        
#         # ×™×¦×™×¨×ª schema ××•×¨×›×‘×ª
#         complex_schema = {
#             "device_temp": {"type": "float", "bits": 32, "min": -40.0, "max": 120.0},
#             "cpu_usage": {"type": "int", "bits": 8},
#             "memory_usage": {"type": "int", "bits": 8}, 
#             "network_status": {"type": "enum", "values": [0, 1, 2, 3]},
#             "is_online": {"type": "bool"},
#             "hostname": {"type": "string", "length": 12, "compressed": True},
#             "uptime": {"type": "time", "bits": 32, "range": 86400},
#             "error_count": {"type": "int", "bits": 16}
#         }
        
#         schema_file = os.path.join(self.temp_dir, "integration_schema.json")
#         with open(schema_file, 'w') as f:
#             json.dump(complex_schema, f)
        
#         # ×™×¦×™×¨×ª ××—×•×œ×œ
#         generator = EnhancedTelemetryGeneratorPro(
#             schema_file,
#             output_dir=self.temp_dir,
#             enable_gpu=True
#         )
        
#         # ×”×“×¤×¡×ª ××™×“×¢ ×¢×œ ×”×¡×›××”
#         schema_info = generator.get_enhanced_schema_info()
#         print(f"ğŸ“Š Schema info: {schema_info['data_fields_count']} fields, "
#               f"{schema_info['bytes_per_record']} bytes/record")
        
#         # ×™×¦×™×¨×ª ×§×‘×¦×™× ×‘×¤×•×¨××˜×™× ×©×•× ×™×
#         test_cases = [
#             (OutputFormat.BINARY, 10000, "high_volume"),
#             (OutputFormat.JSON, 1000, "debug_data"), 
#             (OutputFormat.INFLUX_LINE, 2000, "influx_import")
#         ]
        
#         all_paths = []
        
#         for format_type, num_records, prefix in test_cases:
#             print(f"ğŸ“ ×™×•×¦×¨ {num_records} records ×‘×¤×•×¨××˜ {format_type.value}...")
            
#             # ×”×¢×¨×›×ª ×“×¨×™×©×•×ª ××—×¡×•×Ÿ
#             storage = generator.estimate_storage_requirements(
#                 num_records, format_type, compression_ratio=0.6
#             )
#             print(f"   ğŸ’¾ ×¦×¤×•×™: {storage['compressed_mb']:.1f} MB")
            
#             # ×™×¦×™×¨×” ×‘×¤×•×¢×œ
#             paths = generator.generate_multiple_files_enhanced(
#                 num_files=3,
#                 records_per_file=num_records // 3,
#                 file_prefix=prefix,
#                 output_format=format_type,
#                 use_gpu_batches=True,
#                 batch_size=500,
#                 record_type_ratio={
#                     RecordType.UPDATE: 0.7,
#                     RecordType.EVENT: 0.3
#                 }
#             )
            
#             all_paths.extend(paths)
            
#             # ×‘×“×™×§×ª ×”×ª×•×¦××•×ª
#             total_size = sum(os.path.getsize(path) for path in paths)
#             actual_mb = total_size / (1024 * 1024)
#             print(f"   âœ… ×™×¦×¨ {len(paths)} ×§×‘×¦×™×, ×’×•×“×œ: {actual_mb:.1f} MB")
        
#         # ×‘×“×™×§×ª benchmark
#         print("\nâš¡ ××¨×™×¥ benchmark...")
#         benchmark_results = generator.benchmark_generation_speed(
#             test_records=5000,
#             use_gpu=True,
#             batch_size=1000
#         )
        
#         for metric, value in benchmark_results.items():
#             print(f"   {metric}: {value:.1f}")
        
#         print(f"\nâœ… Pipeline test ×”×•×©×œ× ×‘×”×¦×œ×—×”!")
#         print(f"   ğŸ“ ×™×¦×¨ {len(all_paths)} ×§×‘×¦×™×")
#         print(f"   ğŸ“Š ×¡×”×´×› {sum(len(complex_schema) for _ in range(3))} * ×¨×©×•××•×ª")
        
#         return True


# if __name__ == "__main__":
#     import tempfile
#     import shutil
    
#     # ×™×¦×™×¨×ª ×ª×™×§×™×” ×–×× ×™×ª
#     temp_dir = tempfile.mkdtemp()
    
#     try:
#         print("ğŸ§ª ××ª×—×™×œ ×‘×“×™×§×•×ª ××§×™×¤×•×ª...")
        
#         # ×‘×“×™×§×•×ª ×™×—×™×“×”
#         unit_success = run_comprehensive_enhanced_tests()
        
#         if unit_success:
#             print("\n" + "="*60)
#             # ×‘×“×™×§×•×ª ××™× ×˜×’×¨×¦×™×”
#             integration_test = IntegrationTest(temp_dir)
#             integration_success = integration_test.test_full_pipeline()
            
#             if integration_success:
#                 print("\nğŸ‰ ×›×œ ×”×‘×“×™×§×•×ª ×¢×‘×¨×• ×‘×”×¦×œ×—×”!")
#                 print("âœ… ×”×§×•×“ ××•×›×Ÿ ×œ×©×™××•×© ×‘×¤×¨×•×“×§×¦×™×”!")
#             else:
#                 print("\nâŒ ×‘×“×™×§×•×ª ×”××™× ×˜×’×¨×¦×™×” × ×›×©×œ×•")
#         else:
#             print("\nâŒ ×‘×“×™×§×•×ª ×”×™×—×™×“×” × ×›×©×œ×• - ×¦×¨×™×š ×œ×ª×§×Ÿ")
    
#     finally:
#         # × ×™×§×•×™
#         shutil.rmtree(temp_dir)
#         print(f"\nğŸ§¹ × ×•×§×” ×ª×™×§×™×” ×–×× ×™×ª: {temp_dir}")
import os
import json
import tempfile
import unittest
import shutil
import struct
import time
from typing import Dict, Any
import logging

# ×™×™×‘×•× ×”×§×œ××¡×™× ×©×œ× ×• - ×¢×“×›× ×™ ×œ×¤×™ ×”×§×‘×¦×™× ×©×œ×š
from generator import (
    EnhancedBitPacker,
    InfluxDBLineProtocolWriter,
    EnhancedTelemetryGeneratorPro,
    RecordType,
    OutputFormat,
    TelemetryRecord
)

class TestInfluxDBLineProtocol(unittest.TestCase):
    """×‘×“×™×§×•×ª ×œ×¤×•×¨××˜ InfluxDB Line Protocol"""
    
    def test_basic_line_format(self):
        """×‘×“×™×§×ª ×¤×•×¨××˜ ×‘×¡×™×¡×™"""
        measurement = "temperature"
        tags = {"sensor": "A", "location": "room1"}
        fields = {"value": 23.5, "status": True}
        timestamp = 1640995200000000000
        
        line = InfluxDBLineProtocolWriter.format_record(
            measurement, tags, fields, timestamp
        )
        
        # ×‘×“×™×§×” ×©×”×¤×•×¨××˜ × ×›×•×Ÿ
        self.assertIn("temperature,sensor=A,location=room1", line)
        # ×ª×™×§×•×Ÿ: boolean True -> "true" ×‘-InfluxDB
        self.assertIn('value=23.5,status=true', line)  # ×ª×•×§×Ÿ ×-false ×œ-true
        self.assertIn(str(timestamp), line)

    def test_string_field_quoting(self):
        """×‘×“×™×§×ª ×¦×™×˜×•×˜ ×©×œ string fields"""
        measurement = "logs"
        tags = {}
        fields = {"message": "Hello World", "level": "info"}
        timestamp = 0
        
        line = InfluxDBLineProtocolWriter.format_record(
            measurement, tags, fields, timestamp
        )
        
        self.assertIn('"Hello World"', line)
        self.assertIn('"info"', line)

    def test_integer_field_suffix(self):
        """×‘×“×™×§×ª ×¡×™×•××ª i ×œ×©×“×•×ª int"""
        measurement = "counter"
        tags = {}
        fields = {"count": 42}
        timestamp = 0
        
        line = InfluxDBLineProtocolWriter.format_record(
            measurement, tags, fields, timestamp
        )
        
        self.assertIn('count=42i', line)


class TestEnhancedTelemetryGenerator(unittest.TestCase):
    """×‘×“×™×§×•×ª ××ª×§×“××•×ª ×œ××—×œ×§×ª ×”×¨××©×™×ª"""
    
    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()
        self.schema_file = os.path.join(self.temp_dir, "enhanced_schema.json")
        
        # Schema ××ª×§×“××ª ×¢× enum strings
        self.enhanced_schema = {
            "temperature": {"type": "float", "bits": 32, "min": -50.0, "max": 150.0},
            "pressure": {"type": "int", "bits": 16},
            "device_id": {"type": "string", "length": 8, "max_length": 16},
            "status": {"type": "bool"},
            "mode": {"type": "enum", "values": [0, 1, 2, 3, 4]},
            "alert_level": {"type": "enum", "values": ["none", "low", "medium", "high"]},  # String enum
            "timestamp_offset": {"type": "time", "bits": 16, "range": 3600}
        }
        
        with open(self.schema_file, 'w') as f:
            json.dump(self.enhanced_schema, f)
    
    def tearDown(self):
        shutil.rmtree(self.temp_dir)
    
    def test_enhanced_record_generation(self):
        """×‘×“×™×§×ª ×™×¦×™×¨×ª ×¨×©×•××” ××ª×§×“××ª"""
        generator = EnhancedTelemetryGeneratorPro(
            self.schema_file, 
            output_dir=self.temp_dir
        )
        
        record = generator.generate_enhanced_record()
        
        # ×‘×“×™×§×•×ª ×‘×¡×™×¡×™×•×ª
        self.assertIsInstance(record, TelemetryRecord)
        self.assertIn(record.record_type, [RecordType.UPDATE, RecordType.EVENT])
        self.assertGreater(record.timestamp, 0)
        self.assertGreaterEqual(record.sequence_id, 0)
        
        # ×‘×“×™×§×ª × ×ª×•× ×™×
        self.assertIn("temperature", record.data)
        self.assertIn("device_id", record.data)
        self.assertIn("alert_level", record.data)  # ×‘×“×™×§×ª string enum
        
        # ×‘×“×™×§×ª ×˜×•×•×—×™×
        temp = record.data["temperature"]
        self.assertGreaterEqual(temp, -50.0)
        self.assertLessEqual(temp, 150.0)
        
        device_id = record.data["device_id"]
        self.assertIsInstance(device_id, str)
        self.assertLessEqual(len(device_id), 16)
        
        # ×‘×“×™×§×ª string enum
        alert = record.data["alert_level"]
        self.assertIn(alert, ["none", "low", "medium", "high"])

    def test_string_enum_handling(self):
        """×‘×“×™×§×” ××™×•×—×“×ª ×œ×˜×™×¤×•×œ ×‘-string enums"""
        generator = EnhancedTelemetryGeneratorPro(
            self.schema_file,
            output_dir=self.temp_dir
        )
        
        # ×™×¦×™×¨×ª ×”×¨×‘×” ×¨×©×•××•×ª ×œ×‘×“×™×§×ª ×”×ª×¤×œ×’×•×ª
        records = [generator.generate_enhanced_record() for _ in range(100)]
        
        alert_values = [r.data["alert_level"] for r in records]
        unique_alerts = set(alert_values)
        
        # ×‘×“×™×§×” ×©×›×œ ×”×¢×¨×›×™× ×ª×§×™× ×™×
        expected_values = {"none", "low", "medium", "high"}
        self.assertTrue(unique_alerts.issubset(expected_values))
        
        # ×‘×“×™×§×” ×©×”×ª×¤×œ×’×•×ª ×¡×‘×™×¨×” (×œ× ×›×•×œ× ××•×ª×• ×¢×¨×š)
        self.assertGreater(len(unique_alerts), 1)

    def test_record_type_ratios(self):
        """×‘×“×™×§×ª ×™×—×¡ ×¡×•×’×™ ×¨×©×•××•×ª"""
        generator = EnhancedTelemetryGeneratorPro(
            self.schema_file,
            output_dir=self.temp_dir
        )
        
        # ×™×¦×™×¨×ª ×”×¨×‘×” ×¨×©×•××•×ª ×¢× ×™×—×¡ ××•×’×“×¨
        ratio = {RecordType.UPDATE: 0.8, RecordType.EVENT: 0.2}
        file_path = os.path.join(self.temp_dir, "ratio_test.json")
        
        generator.write_records_enhanced(
            file_path,
            1000,
            output_format=OutputFormat.JSON,
            record_type_ratio=ratio
        )
        
        # ×¡×¤×™×¨×ª ×¡×•×’×™ ×¨×©×•××•×ª
        update_count = 0
        event_count = 0
        
        with open(file_path, 'r') as f:
            for line in f:
                data = json.loads(line.strip())
                if data['type'] == 'update':
                    update_count += 1
                else:
                    event_count += 1
        
        total = update_count + event_count
        update_ratio = update_count / total
        event_ratio = event_count / total
        
        # ×‘×“×™×§×” ×©×”×™×—×¡ ×§×¨×•×‘ ×œ××¦×•×¤×” (Â±10%)
        self.assertAlmostEqual(update_ratio, 0.8, delta=0.1)
        self.assertAlmostEqual(event_ratio, 0.2, delta=0.1)

    def test_multiple_output_formats(self):
        """×‘×“×™×§×ª ×¤×•×¨××˜×™ ×¤×œ×˜ ×©×•× ×™×"""
        generator = EnhancedTelemetryGeneratorPro(
            self.schema_file,
            output_dir=self.temp_dir
        )
        
        test_cases = [
            (OutputFormat.BINARY, ".bin"),
            (OutputFormat.JSON, ".json"),
            (OutputFormat.INFLUX_LINE, ".txt")
        ]
        
        for format_type, extension in test_cases:
            with self.subTest(format=format_type):
                file_path = os.path.join(self.temp_dir, f"test{extension}")
                generator.write_records_enhanced(
                    file_path,
                    100,
                    output_format=format_type
                )
                
                self.assertTrue(os.path.exists(file_path))
                self.assertGreater(os.path.getsize(file_path), 0)

    def test_enhanced_parallel_generation(self):
        """×‘×“×™×§×ª ×™×¦×™×¨×” ××§×‘×™×œ×™×ª ××ª×§×“××ª"""
        generator = EnhancedTelemetryGeneratorPro(
            self.schema_file,
            output_dir=self.temp_dir
        )
        
        paths = generator.generate_multiple_files_enhanced(
            num_files=3,
            records_per_file=100,
            output_format=OutputFormat.JSON,
            record_type_ratio={RecordType.UPDATE: 0.5, RecordType.EVENT: 0.5}
        )
        
        self.assertEqual(len(paths), 3)
        
        for path in paths:
            self.assertTrue(os.path.exists(path))
            self.assertGreater(os.path.getsize(path), 0)
            
            # ×‘×“×™×§×” ×©×”×§×•×‘×¥ ××›×™×œ JSON ×ª×§×™×Ÿ
            with open(path, 'r') as f:
                line_count = 0
                for line in f:
                    data = json.loads(line.strip())
                    self.assertIn('type', data)
                    self.assertIn('timestamp', data)
                    self.assertIn('data', data)
                    line_count += 1
                
                self.assertEqual(line_count, 100)

    def test_storage_estimation(self):
        """×‘×“×™×§×ª ×”×¢×¨×›×ª ×“×¨×™×©×•×ª ××—×¡×•×Ÿ"""
        generator = EnhancedTelemetryGeneratorPro(
            self.schema_file,
            output_dir=self.temp_dir
        )
        
        # ×‘×“×™×§×” ×œ×¤×•×¨××˜×™× ×©×•× ×™×
        for format_type in OutputFormat:
            with self.subTest(format=format_type):
                estimation = generator.estimate_storage_requirements(
                    10000, 
                    format_type,
                    compression_ratio=0.7
                )
                
                self.assertEqual(estimation["records"], 10000)
                self.assertEqual(estimation["format"], format_type.value)
                self.assertGreater(estimation["uncompressed_bytes"], 0)
                self.assertLess(estimation["compressed_bytes"], estimation["uncompressed_bytes"])
                self.assertEqual(estimation["compression_ratio"], 0.7)

    def test_data_validation(self):
        """×‘×“×™×§×ª ×•×•×œ×™×“×¦×™×” ×©×œ × ×ª×•× ×™× - ×¨×§ ×¢×¨×›×™× ×—×•×§×™×™×"""
        generator = EnhancedTelemetryGeneratorPro(
            self.schema_file,
            output_dir=self.temp_dir
        )
        
        # ×™×¦×™×¨×ª 1000 ×¨×©×•××•×ª ×œ×‘×“×™×§×” ×¡×˜×˜×™×¡×˜×™×ª
        records = []
        for _ in range(1000):
            record = generator.generate_enhanced_record()
            records.append(record)
        
        # ×‘×“×™×§×ª ×›×œ ×”×¨×©×•××•×ª
        for record in records:
            data = record.data
            
            # ×˜××¤×¨×˜×•×¨×” ×‘×˜×•×•×—
            temp = data["temperature"]
            self.assertGreaterEqual(temp, -50.0, f"Temperature {temp} below minimum")
            self.assertLessEqual(temp, 150.0, f"Temperature {temp} above maximum")
            
            # ×œ×—×¥ ×‘×˜×•×•×— ×ª×§×™×Ÿ
            pressure = data["pressure"]
            self.assertGreaterEqual(pressure, 0, f"Pressure {pressure} negative")
            self.assertLessEqual(pressure, 65535, f"Pressure {pressure} above 16-bit max")
            
            # String enum ×ª×§×™×Ÿ
            alert = data["alert_level"]
            self.assertIn(alert, ["none", "low", "medium", "high"], f"Invalid alert level: {alert}")
            
            # Int enum ×ª×§×™×Ÿ
            mode = data["mode"]
            self.assertIn(mode, [0, 1, 2, 3, 4], f"Invalid mode: {mode}")
            
            # Boolean ×ª×§×™×Ÿ
            status = data["status"]
            self.assertIsInstance(status, bool, f"Status not boolean: {status}")
            
            # String ID ×ª×§×™×Ÿ
            device_id = data["device_id"]
            self.assertIsInstance(device_id, str, f"Device ID not string: {device_id}")
            self.assertLessEqual(len(device_id), 16, f"Device ID too long: {device_id}")


if __name__ == "__main__":
    unittest.main(verbosity=2)